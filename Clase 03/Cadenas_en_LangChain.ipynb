{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "959YRfTqWTts"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain-community langchain-core langchain-openai httpx==0.27.2\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "HEOkrObwYZMy"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import OpenAI #Clase base para interactuar con OpenAI (ver ant deprecado from langchain.llms import OpenAI)\n",
        "from langchain.chains import LLMChain #Clase base para crear cadenas\n",
        "from langchain.prompts import PromptTemplate #se utiliza para definir plantillas de prompts reutilizables, con variables dinámicas\n",
        "from langchain.prompts import ChatPromptTemplate #clase diseñada para trabajar específicamente con modelos de chat en modo de conversación."
      ],
      "metadata": {
        "id": "nByzseT6YfZX"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#importamos la api\n",
        "with open(\"/content/api_key.txt\") as archivo:\n",
        "  apikey = archivo.read()\n",
        "os.environ[\"OPENAI_API_KEY\"] = apikey"
      ],
      "metadata": {
        "id": "EuA9zi3nZURw"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "3BWCl6PLseJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modelo de cadena de texto\n",
        "\n",
        "- Modelos antiguos Estos modelos más antiguos aceptan una cadena de texto como entrada y devuelven una cadena de texto como salida\n",
        "- Modelos con capacidad de conversación (\"Chat\"): Estos modelos avanzados son diseñados específicamente para interacciones conversacionales"
      ],
      "metadata": {
        "id": "K8-ivsZ-m2eQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejemplo deprecado mediante OpenAI usando un modelo antigu modo no conversacional soporte gpt-3.5-turbo-instruct\n",
        "\n",
        "llm = OpenAI(temperature=0.5, model=\"gpt-3.5-turbo-instruct\")\n",
        "\n",
        "response = llm.invoke(\"¿Cual es la capital de Peru?\")\n",
        "\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jaR1Mf28aUli",
        "outputId": "a2f485cf-ce2c-445e-8e40-13176c2f8279"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "La capital de Perú es Lima.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "m9dSYvaAsYFD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PromptTemplate\n",
        "\n",
        "Diseñado para crear prompts dinámicos con variables, generalmente para tareas no conversacionales."
      ],
      "metadata": {
        "id": "VIjEhFtfryVV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "#Template\n",
        "prompt_template =PromptTemplate.from_template(\"Describe el siguiente producto de una forma convincente y breve {product_name}\")\n",
        "\n",
        "#instancia\n",
        "llm = ChatOpenAI(temperature=0.5)\n",
        "\n",
        "#cadena\n",
        "chain = LLMChain(llm=llm, prompt=prompt_template)\n",
        "\n",
        "#ejecutamos\n",
        "input = {'product_name':'Botella de agua ecologica'}\n",
        "print(chain.run(input))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XFWP--2kn8Rf",
        "outputId": "c25e7207-40ee-4774-f271-b6e99e18c238"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Esta botella de agua ecológica es la elección perfecta para aquellos que buscan reducir su impacto ambiental. Hecha de materiales reciclados y reciclables, esta botella es una alternativa sostenible a las botellas de plástico desechables. Con un diseño elegante y práctico, es ideal para llevar contigo a todas partes y mantenerte hidratado de forma responsable. ¡Haz tu parte por el planeta y elige esta botella de agua ecológica!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "F2wS3eJRsaJZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ChatPromptTemplate\n",
        "Enfocado en la creación de prompts para modelos en formato de chat, como GPT-4 en modo conversacional."
      ],
      "metadata": {
        "id": "ecdSpa6lsGv8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "from langchain.prompts import (\n",
        "    ChatPromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate\n",
        ")\n",
        "\n",
        "# Crear las plantillas de mensajes\n",
        "system_message = SystemMessagePromptTemplate.from_template(\n",
        "    \"Eres un asistente que genera una descripción de un producto de forma convincente y breve.\"\n",
        ")\n",
        "human_message = HumanMessagePromptTemplate.from_template(\"{product_name}\")\n",
        "\n",
        "# Crear el prompt de chat\n",
        "chat_prompt = ChatPromptTemplate.from_messages([system_message, human_message])\n",
        "\n",
        "# Instanciar el modelo de chat\n",
        "llm = ChatOpenAI(temperature=0.5)\n",
        "\n",
        "# Crear la cadena\n",
        "chain = LLMChain(llm=llm, prompt=chat_prompt)\n",
        "\n",
        "# Ejecutar la cadena\n",
        "input_data = {'product_name': 'Botella de agua ecológica'}\n",
        "print(chain.run(input_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CfdX0a3MqoKt",
        "outputId": "c2f19973-d3d4-4e16-ce21-2841779d6259"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "¡Descubre nuestra botella de agua ecológica, la compañera perfecta para tus actividades diarias! Fabricada con materiales resistentes y amigables con el medio ambiente, esta botella te permite mantenerte hidratado/a de forma sostenible. Su diseño moderno y ergonómico la hace fácil de transportar a donde vayas. ¡Cuida de ti y del planeta con nuestra botella de agua ecológica!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cadenas de secuencia simple"
      ],
      "metadata": {
        "id": "p4Jm9QwktGEw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SimpleSequentialChain"
      ],
      "metadata": {
        "id": "9dJB3-vExMvV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import SimpleSequentialChain\n",
        "\n",
        "llm = ChatOpenAI(temperature=0.5)\n",
        "#cadena 01\n",
        "first_template = ChatPromptTemplate.from_template(\"\"\"\n",
        "Indica si el texto denota una emocion en una sola palabra (positiva o Negativa): {mensaje}\"\"\")\n",
        "chain_one = LLMChain(llm=llm, prompt=first_template)\n",
        "\n",
        "#cadena 02\n",
        "second_template =  ChatPromptTemplate.from_template(\"\"\"\n",
        "Envia un mensaje de agradecimiento o Disculpa si el resultado es positivo o Negativo:  {emocion}\n",
        "\"\"\")\n",
        "chain_two = LLMChain(llm=llm, prompt=second_template)\n",
        "\n",
        "#Secuencia lineal\n",
        "cadena_lineal = SimpleSequentialChain(chains=[chain_one, chain_two])\n",
        "resultado = cadena_lineal.run(\"Su producto se malogro al dia siguiente\")\n",
        "\n",
        "print(resultado)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdcYQsFGtEYL",
        "outputId": "46d71e23-2c49-4021-aa9b-29719addc4a1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Disculpa por la respuesta negativa, pero te agradezco de todas formas por tomarte el tiempo de considerar mi solicitud. ¡Gracias!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SequentialChain"
      ],
      "metadata": {
        "id": "HUEF37rNxO1w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import SequentialChain\n",
        "\n",
        "llm = ChatOpenAI(temperature=0.5)\n",
        "# cadena 01 | input: reseña | output: spanish_rev\n",
        "one_template = ChatPromptTemplate.from_template(\"\"\"\n",
        "Traducir la siguiente reseña en español {review}\n",
        "\"\"\")\n",
        "chain_one = LLMChain(llm=llm, prompt=one_template, output_key=\"spanish_rev\")\n",
        "\n",
        "# Cadena 02 | input spanish_rev | output resumen\n",
        "two_template = ChatPromptTemplate.from_template(\"\"\"\n",
        "resume la siguiente reseña en una frase {spanish_rev}\n",
        "\"\"\")\n",
        "chain_two = LLMChain(llm=llm, prompt=two_template, output_key=\"resumen\")\n",
        "\n",
        "#cadena 03 | input review | output lenguaje\n",
        "three_template = ChatPromptTemplate.from_template(\"\"\"\n",
        "En que idioma se encuentra la siguiente reseña {review}\n",
        "\"\"\")\n",
        "chain_three = LLMChain(llm=llm, prompt=three_template, output_key=\"lenguaje\")\n",
        "\n",
        "#cadena 04 | input resumen,lenguaje | output mensaje_seg\n",
        "four_template = ChatPromptTemplate.from_template(\"\"\"\n",
        "Escribe una respuesta de seguimiento a el siguiente resumen en el idioma especificado {resumen}, idioma: {lenguaje}\n",
        "\"\"\")\n",
        "chain_four = LLMChain(llm=llm, prompt=four_template, output_key=\"mensaje_seg\")\n",
        "\n",
        "#flujo de cadena\n",
        "\n",
        "cadena_secuencial = SequentialChain(\n",
        "    chains=[chain_one, chain_two, chain_three, chain_four],\n",
        "    input_variables=[\"review\"],\n",
        "    output_variables=[\"spanish_rev\", \"resumen\", \"lenguaje\", \"mensaje_seg\"]\n",
        ")"
      ],
      "metadata": {
        "id": "SZHTOehIc39T"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "review = \"\"\"For all their convenience, Bluetooth headphones and earbuds have fundamental problems.\n",
        "            Take their batteries (please). They're only fully rechargeable 300–500 times,\n",
        "            which means that after just two or three years of moderate-to-heavy use,\n",
        "            most people toss their depleted wireless ear-fi in a drawer and buy a new pair.\"\"\""
      ],
      "metadata": {
        "id": "uDJP9f1wvfBc"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = cadena_secuencial.invoke(review)"
      ],
      "metadata": {
        "id": "SpbNQLfgzVWP"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Traduccion: \"+ result['spanish_rev'] +\"\\n\\n\"+\n",
        "      \"Resumen: \"+ result['resumen'] +\"\\n\\n\"+\n",
        "      \"Idioma: \"+ result['lenguaje'] +\"\\n\\n\"+\n",
        "      \"Respuesta: \"+ result['mensaje_seg']\n",
        "      )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AvvioKsczY3X",
        "outputId": "847ad7b7-3428-47cf-faff-95f6e3fe01aa"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traduccion: A pesar de toda su conveniencia, los auriculares y audífonos Bluetooth tienen problemas fundamentales. \n",
            "Tomen sus baterías (por favor). Solo se pueden recargar completamente de 300 a 500 veces, \n",
            "lo que significa que después de solo dos o tres años de uso moderado a intenso, \n",
            "la mayoría de las personas tiran sus agotados auriculares inalámbricos en un cajón y compran un par nuevo.\n",
            "\n",
            "Resumen: Los auriculares Bluetooth tienen problemas de durabilidad debido a la limitada vida útil de sus baterías.\n",
            "\n",
            "Idioma: La reseña se encuentra en inglés.\n",
            "\n",
            "Respuesta: Thank you for bringing up the issue of durability with Bluetooth headphones. It is true that the limited lifespan of their batteries can be a significant concern. It is important for manufacturers to address this issue and work towards improving the overall longevity of their products. In the meantime, it may be helpful for consumers to research and consider purchasing headphones with replaceable batteries or extended warranty options.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cadena Condicional"
      ],
      "metadata": {
        "id": "CjZ3QoSP11Rr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(temperature=0.5)\n",
        "\n",
        "rw=\"Tengo un problema con los audifonos que me vendieron, no funcionan desde el primer dia\"\n",
        "\n",
        "first_prompt = ChatPromptTemplate.from_template(\n",
        "    \"\"\"analiza el sentimiento de la siguiente reseña\n",
        "    y devuelve el resultado en una sola palabra, positivo o negativo {review}\"\"\"\n",
        "    )\n",
        "\n",
        "second_prompt = ChatPromptTemplate.from_template(\n",
        "    \"\"\"Genera una respuesta de agradecimiento para la siguiente reseña positiva {review}\"\"\")\n",
        "\n",
        "third_prompt = ChatPromptTemplate.from_template(\n",
        "    \"\"\"Genera una respuesta de disculpas para la siguiente reseña negativa {review}\"\"\")\n",
        "\n",
        "chain_one = LLMChain(llm=llm, prompt=first_prompt,\n",
        "                     output_key= \"sentiment\")\n",
        "result_one = chain_one.run({\"review\": rw})\n",
        "\n",
        "#cadena de respuesta condicional\n",
        "\n",
        "if \"positivo\" in result_one.lower():\n",
        "  chain_three = LLMChain(llm=llm, prompt=second_prompt,\n",
        "                     output_key= \"response\")\n",
        "elif \"negativo\" in result_one.lower():\n",
        "  chain_three = LLMChain(llm=llm, prompt=third_prompt,\n",
        "                     output_key= \"response\")\n",
        "  result_three = chain_three.run({\"review\": rw})\n",
        "\n",
        "  print(result_three)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RHBmJ_0EznGk",
        "outputId": "138839a5-874b-47a2-b232-cf1a7d57e118"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lamentamos mucho escuchar sobre tu mala experiencia con los audífonos que adquiriste. Nos disculpamos sinceramente por cualquier inconveniente que esto te haya causado. Nos gustaría resolver este problema lo antes posible. Por favor, ponte en contacto con nuestro servicio de atención al cliente para que podamos encontrar una solución adecuada para ti. ¡Gracias por tu retroalimentación!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cadena Condicional como funcion"
      ],
      "metadata": {
        "id": "UJtJpj5R7Nt3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main_chain(review, client):\n",
        "\n",
        "  first_prompt = ChatPromptTemplate.from_template(\n",
        "    \"\"\"analiza el sentimiento de la siguiente reseña\n",
        "    y devuelve el resultado en una sola palabra, positivo o negativo {review}\"\"\"\n",
        "    )\n",
        "\n",
        "  second_prompt = ChatPromptTemplate.from_template(\n",
        "    \"\"\"Genera una respuesta de agradecimiento para {client} en formato de correo para la siguiente reseña positiva {review}\"\"\")\n",
        "\n",
        "  third_prompt = ChatPromptTemplate.from_template(\n",
        "    \"\"\"Genera una respuesta de disculpas para {client} en formato de correo para la siguiente reseña negativa {review}\"\"\")\n",
        "\n",
        "  #cadena 1 analisis de sentimiento\n",
        "  chain_one = LLMChain(llm=llm, prompt=first_prompt,\n",
        "                     output_key= \"sentiment\")\n",
        "  result_one = chain_one.run({\"review\": rw})\n",
        "\n",
        "  #cadena 3 condicional\n",
        "  if \"positivo\" in result_one.lower():\n",
        "    chain_two = LLMChain(llm=llm, prompt=second_prompt,\n",
        "                     output_key= \"response\")\n",
        "  elif \"negativo\" in result_one.lower():\n",
        "    chain_two = LLMChain(llm=llm, prompt=third_prompt,\n",
        "                     output_key= \"response\")\n",
        "    result_chain = chain_two.run({\"review\": rw, \"client\": client})\n",
        "  return result_chain"
      ],
      "metadata": {
        "id": "JywzC1rv3ghL"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "review = \"\"\"For all their convenience, Bluetooth headphones and earbuds have fundamental problems.\n",
        "            Take their batteries (please). They're only fully rechargeable 300–500 times,\n",
        "            which means that after just two or three years of moderate-to-heavy use,\n",
        "            most people toss their depleted wireless ear-fi in a drawer and buy a new pair.\"\"\"\n",
        "client= \"Javier Orihuela\"\n",
        "\n",
        "result = main_chain(review,client)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zaCK3e4p4YmH",
        "outputId": "b4cee1da-1324-45f2-cdd1-35c03c2360f2"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estimado Javier Orihuela,\n",
            "\n",
            "Lamentamos profundamente escuchar que has tenido problemas con los audífonos que adquiriste en nuestra tienda. Nos disculpamos sinceramente por cualquier inconveniente que esto te haya causado.\n",
            "\n",
            "Queremos que sepas que la satisfacción de nuestros clientes es nuestra máxima prioridad y estamos comprometidos a resolver cualquier problema que puedas tener con nuestros productos. Por favor, háznoslo saber y haremos todo lo posible para encontrar una solución que te satisfaga.\n",
            "\n",
            "Agradecemos tu retroalimentación y la oportunidad de mejorar nuestros servicios. Esperamos poder recuperar tu confianza en nuestra marca y que puedas disfrutar de una experiencia positiva con nuestros productos en el futuro.\n",
            "\n",
            "Gracias por tu comprensión y paciencia.\n",
            "\n",
            "Atentamente,\n",
            "\n",
            "[Nombre de la empresa]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KUcQEI3KYa17"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}