{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Primeros pasos\n",
        "### Nos conectamos mendiante la API de openAI para usar su modelo LLM (GPT)"
      ],
      "metadata": {
        "id": "PFhcfMIqFVXz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "eM3_cmCIFUF7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "import sys"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### por seguridad importamos la API de un archivo en lugar de ponerlo directamente en el codigo fuente"
      ],
      "metadata": {
        "id": "eC9ZXK_uOHh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"api_key.txt\") as archivo:\n",
        "    openai.api_key = archivo.readline()"
      ],
      "metadata": {
        "id": "jZ3RRJ70FhhX"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creamos una variable vacia donde almacenaremos la memoria\n",
        "contexto = []"
      ],
      "metadata": {
        "id": "-CMomNdRGlWi"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Principales roles de prompts\n",
        "- system: Establece el comportamiento y las instrucciones principales del modelo, actuando como guía durante toda la conversación.\n",
        "\n",
        "- user: Representa las entradas del usuario, donde se plantean preguntas, solicitudes o instrucciones.\n",
        "\n",
        "- assistant: Corresponde a las respuestas generadas por el modelo en función de las instrucciones del rol system y las entradas del rol user."
      ],
      "metadata": {
        "id": "w5Z0Bo-0NxAn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creamos un prompt con el rol system que delimite lo que puede hacer el modelo\n",
        "\n",
        "#caracteristicas de un prompt consistente\n",
        "\n",
        "#### Ser conciso y puntual\n",
        "#### Delimitar una tarea especifica\n",
        "#### Evitar redundancia\n",
        "\n",
        "\n",
        "prompt_system = \"\"\"Eres un experto en matematicas, solo puedes responder preguntas matematicas,\n",
        "                    para cualquier otra pregunta indicar que no conoces la respuesta\"\"\"\n",
        "\n",
        "contexto.append({'role':'system', 'content':f\"\"\" {prompt_system}\"\"\"})\n"
      ],
      "metadata": {
        "id": "fR4oyj0CGMiy"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creamos una funcion que\n",
        "\n",
        "## se encargue de enviar mensajes, modelo y la temperatura mediante el API de OpenAI\n",
        "\n",
        "def enviar_mensajes(messages, model=\"gpt-4\", temperature=0):\n",
        "    response = openai.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=temperature,\n",
        "    )\n",
        "    return response.choices[0].message.content"
      ],
      "metadata": {
        "id": "K9ss_KR7Kq7L"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creamos una funcion que:\n",
        "\n",
        "## Agrega a la memoria el mensaje\n",
        "## utiliza la funcion enviar_mensajes para la integracion con el API\n",
        "## agrega la respuesta al contexto\n",
        "## imprime la respuesta\n",
        "\n",
        "def recargar_mensajes(charla):\n",
        "    contexto.append({'role':'user', 'content':f\"{charla}\"})\n",
        "    response = enviar_mensajes(contexto,temperature=0.7)\n",
        "    contexto.append({'role':'assistant','content':f\"{response}\"})\n",
        "    print()\n",
        "    print(response)"
      ],
      "metadata": {
        "id": "mZd-_ZLCIoJ7"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mensaje = \"3+5\"\n",
        "recargar_mensajes(mensaje)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2grLHtWoJQo8",
        "outputId": "7f681282-7fa8-4304-a268-98ac9d2f4971"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "El resultado de sumar 3 y 5 es 8.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mensaje = \"multiplicalo x 2\"\n",
        "recargar_mensajes(mensaje)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MjbbBmHbJ5nl",
        "outputId": "22c3d8e9-4656-4177-c197-8b0a1ae696b5"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "El resultado de multiplicar 8 por 2 es 16.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UXfku6GQMAYW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}