{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8TDnd_SiNb0X"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain-community langchain-core langchain-openai httpx==0.27.2 langchain-anthropic langchain-google-genai\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "9rrZ4zPvQTbp"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Dict, Any\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_anthropic import  Anthropic\n",
        "from langchain_core.language_models import BaseLanguageModel\n",
        "from langchain_core.outputs import LLMResult\n",
        "from langchain.schema.messages import AIMessage, BaseMessage\n",
        "from openai import OpenAIError, AuthenticationError\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_openai.chat_models.base import BaseChatOpenAI"
      ],
      "metadata": {
        "id": "h7D741HqQOax"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def deepseek_ai(\n",
        "    model: str = \"deepseek-chat\",\n",
        "    temperature: float = 0.5,\n",
        "    max_tokens: int = 1024,\n",
        "    **kwargs\n",
        ") -> BaseChatOpenAI:\n",
        "    \"\"\"\n",
        "    Crea una instancia de Deepseek configurada para Langchain.\n",
        "\n",
        "    Args:\n",
        "        model (str): Nombre del modelo a usar\n",
        "        temperature (float): Creatividad de las respuestas (0-1)\n",
        "        max_tokens (int): Máximo de tokens en la respuesta\n",
        "        **kwargs: Parámetros adicionales\n",
        "\n",
        "    Returns:\n",
        "        BaseChatOpenAI: Instancia configurada de Deepseek\n",
        "    \"\"\"\n",
        "    return BaseChatOpenAI(\n",
        "        model=model,\n",
        "        openai_api_key=os.environ.get(\"DEEPSEEK_API_KEY\"),\n",
        "        openai_api_base=\"https://api.deepseek.com\",\n",
        "        temperature=temperature,\n",
        "        max_tokens=max_tokens,\n",
        "        **kwargs\n",
        "    )"
      ],
      "metadata": {
        "id": "R5awrrYl0Obe"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_fallback_llm(models: List[Dict[str, Any]], temperature: float = 0.5):\n",
        "    \"\"\"\n",
        "    Devuelve una instancia de LLM configurada con el primer modelo funcional de la lista.\n",
        "\n",
        "    Args:\n",
        "        models (List[Dict[str, Any]]): Lista de modelos configurados con:\n",
        "            - \"env_var\": Variable de entorno para la API key.\n",
        "            - \"api_key\": Clave API para el modelo.\n",
        "            - \"class\": Clase del modelo (por ejemplo, ChatOpenAI).\n",
        "            - \"params\": Parámetros adicionales para inicializar el modelo.\n",
        "        temperature (float): Temperatura para los modelos.\n",
        "\n",
        "    Returns:\n",
        "        BaseLanguageModel: Instancia del primer modelo funcional.\n",
        "\n",
        "    Raises:\n",
        "        Exception: Si todos los modelos fallan.\n",
        "    \"\"\"\n",
        "    for model_config in models:\n",
        "        try:\n",
        "            # Configurar la API key en la variable de entorno\n",
        "            os.environ[model_config[\"env_var\"]] = model_config[\"api_key\"]\n",
        "\n",
        "            # Instanciar el modelo\n",
        "            llm = model_config[\"class\"](temperature=temperature, **model_config.get(\"params\", {}))\n",
        "            llm.predict(\"Ping test\")  # Verifica que la API key es válida\n",
        "            # Si la instancia es exitosa, devolverla\n",
        "            return llm\n",
        "        except AuthenticationError as auth_err:\n",
        "            print(f\"Error de autenticación con {model_config['class'].__name__}: {auth_err}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error al inicializar el modelo {model_config['class'].__name__}: {e}\")\n",
        "\n",
        "    # Si todos los modelos fallan, lanzar excepción\n",
        "    raise Exception(\"No se pudo inicializar ningún modelo LLM debido a errores de autenticación u otros fallos.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "5O7nOxg6Tt_0"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/api_key_openai.txt\") as archivo:\n",
        "  apikey_gpt = archivo.read()\n",
        "with open(\"/content/api_key_claude.txt\") as archivo:\n",
        "  apikey_clude = archivo.read()\n",
        "with open(\"/content/api_key_gemini.txt\") as archivo:\n",
        "  apikey_gemini = archivo.read()\n",
        "with open(\"/content/api_key_deepseek.txt\") as archivo:\n",
        "  apikey_deepseek = archivo.read()"
      ],
      "metadata": {
        "id": "9Fqt4NR5XmYr"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models = [\n",
        "    # openAI GPT-4\n",
        "     {\n",
        "        \"env_var\": \"OPENAI_API_KEY\",\n",
        "        \"api_key\": apikey_gpt,\n",
        "        \"class\": ChatOpenAI,\n",
        "        \"params\": {\"model_name\": \"gpt-4\"}\n",
        "    },\n",
        "    # Anthropic - Claude\n",
        "    {\n",
        "        \"env_var\": \"ANTHROPIC_API_KEY\",\n",
        "        \"api_key\": apikey_clude,\n",
        "        \"class\": Anthropic,\n",
        "        \"params\": {\"model\": \"claude-2\"}\n",
        "    },\n",
        "    # Google - Gemini Pro\n",
        "    {\n",
        "        \"env_var\": \"GOOGLE_API_KEY\",\n",
        "        \"api_key\": apikey_gemini,\n",
        "        \"class\": ChatGoogleGenerativeAI,\n",
        "        \"params\": {\"model\": \"gemini-pro\"}\n",
        "    },\n",
        "    # DeepSeek - r1\n",
        "    {\n",
        "        \"env_var\": \"DEEPSEEK_API_KEY\",\n",
        "        \"api_key\": apikey_deepseek,\n",
        "        \"class\": deepseek_ai,\n",
        "        \"params\": {\n",
        "            \"model\": \"deepseek-reasoner\",\n",
        "            \"max_tokens\": 2048\n",
        "        }\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "C2bPcVTCWNrq"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = get_fallback_llm(models, temperature=0.5)"
      ],
      "metadata": {
        "id": "fjBG7-8NWTTu"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Respuesta: \"+llm.predict(\"¿Cuál es la capital de Francia?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tH4dJCO1WYoH",
        "outputId": "5ad14f0d-0122-4273-caea-6db824dda0e0"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Respuesta: La capital de Francia es París.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PR8sGvIo7xrh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}